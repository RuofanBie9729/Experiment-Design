{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment Design\n",
    "This notebook summarizes the protocal of A/B testing experiment design based on course materials from [Udacity A/B Testing course](https://learn.udacity.com/courses/ud257) and [an experiment design project](https://olgabelitskaya.github.io/P7_Design_an_A_B_Test_Overview.html)\n",
    "##### Overviews\n",
    "1. Two situations A/B testing doesn't work well:\n",
    "    - a. when it takes time to get results\n",
    "    - b. A/B testing can't tell you if you are missing something\n",
    "2. Components of experiment Design:\n",
    "    - design metrics \n",
    "        - invariant metrics: metrics doesn't change because of the experiment and can be used for sanity check (randomization check)\n",
    "        - evaluation metrics: metrics used to evaluate the effect of the experiment\n",
    "    - design experiment\n",
    "        - choose subjects: determine unit of diversion\n",
    "        - choose population: determine what is the population of interest\n",
    "        - sample size calculation:\n",
    "            - a. sample size calculation by margin of error: $$n=\\left(\\frac{Z_{1-\\alpha/2}\\sigma}{E}\\right)^2$$ where $Z_{1-\\alpha/2}$ is the Z-score for $\\alpha$ significance level, $\\sigma$ is the standard error, $E$ is the margin of error, e.g. we hope to observe at least 0.1 in difference, then $E = 0.1$\n",
    "            - b. sample size calculation by power: $$n=\\left(\\frac{Z_{1-\\alpha/2} + Z_{\\beta}}{ES}\\right)^2$$ where $Z_{\\beta}$ is the Z-score for $\\beta$ power level. $ES$ is the effect size, usually computed by $ES = \\frac{|\\mu_1-\\mu_2|}{\\sigma}$. One specific exmaple of sample size calculation for comparing proportions between two groups is: $$n = \\frac{(Z_{1-\\alpha/2}\\sqrt{2\\frac{p_1+p_2}{2}\\left(1-\\frac{p_1+p_2}{2}\\right)}+Z_{\\beta}\\sqrt{p_1(1-p_1)+p_2(1-p_2)})^2}{(p_1-p_2)^2}$$\n",
    "        - decide duration\n",
    "            - Duration of the experiment\n",
    "                - long enough exposure to get sufficient sample size\n",
    "                - not too long because the decision can't wait too long and there is cost of experiment\n",
    "                - if weekday is different from weekend, then at least duration of 7 days\n",
    "                - if the period is 7 days, then 28 days duration is better than 1 month\n",
    "            - When to launch the experiment \n",
    "                - semester vs vacation/holiday\n",
    "                - weekday vs weekend\n",
    "            - What fraction of the traffic should be exposed to the experiment\n",
    "                - if the change is hardly to be noticed, e.g. color of button, then it can be exposed to all the traffic\n",
    "                - if the change might influence user experience, e.g. login method, then it is better to keep the experiment exposed to very small amount of traffic\n",
    "                - whether there are other experiment on-going\n",
    "                - whether you are interested in a specific population, e.g. college student users rather than all the users\n",
    "    - Analysis of experiment:\n",
    "        - sanity check: use invariant metric to check whether the control group and experiment group are equivalent in sample size\n",
    "            - hypothesis test: $$H_0: p = 0.5$$, $SE = \\sqrt{\\frac{0.5*0.5}{n_c+n_e}}$\n",
    "            - if fail:\n",
    "                - a. check technical error, e.g. infrastructure, experiment setup\n",
    "                - b. retrospective analysis: use collected data to recreate balanced experiment diversion and understand what is causing the failure\n",
    "                - c. try pre- and post-period experiment design. If both went wrong, probably infrastructure; if only experiment went wrong, probably experiment setup error, e.g. filter to English language only\n",
    "        - single metric used:\n",
    "            - check both statistical significance (p-value) and practical significance (magnitude and direction)\n",
    "            - alert of Simpson's Paradox: e.g. by department the admission rate of male and female are comparable and by some department female admission rate is higher than male; but because more females apply to low admission departments, causing overall female admission rate lower than male admission rate\n",
    "            - Use both parametric hypothesis test and sign test\n",
    "            - After getting test results, slicing the metrics to check for Simpson's Paradox\n",
    "        - multiple metrics used:\n",
    "            - more likely to see random significant results (need to control false positive/significance)\n",
    "            - Assuming $m$ tests are independent: $$\\alpha_{overall} = 1-(1-\\alpha_{per test})^m$$ this gives: $$\\alpha_{overall}\\leq m\\times \\alpha_{per test}$$\n",
    "            - Bonferroni correction (conservative): $$\\alpha_{pertest}=\\frac{\\alpha_{overall}}{m}$$\n",
    "            - Sidak correction: $$\\alpha_{pertest}=1-(1-\\alpha_{overall})^{1/m}$$\n",
    "        - draw conclusions:\n",
    "            - How do you understand the change:\n",
    "                - consider results for both parametric tests and sign tests\n",
    "                - consider both statistical significance and practical significance\n",
    "                - consider results of both single tests and multiple tests\n",
    "                - consider metric values in slicing\n",
    "            - Whether it is worthy to launch:\n",
    "                - cost of the launch\n",
    "                - Potential increase revenue by the launch\n",
    "                - porportion of users that will be benefited by the launch\n",
    "                - whether the launch would benefit a group of users/features but harm another group of users/features\n",
    "            - Sometimes multiple experiments are needed before making a change\n",
    "3. A protocal of experiment design:\n",
    "    - step 1: define problem of interest, population, unit of diversion, potential confounders\n",
    "    - step 2: design metrics, decide what are invariant metrics and what are evaluation metrics. **Clearly list the expected magnitute and direction for each evaluation metrics.**\n",
    "    - step 3: in pre-stage or retrospective study, collect a sample to compute the variability of the **evaluation metrics** for the use of sample size calculation\n",
    "        - analytical variability: $SE = \\sigma$ or $SE = \\sqrt{p(1-p)/n}$\n",
    "            - **Use $SE\\propto \\frac{1}{\\sqrt{n}}$ to transport this variability between different samples**\n",
    "        - empirical variability: empirical SE or bootstrap\n",
    "    - step 4: calculate sample size for **evaluation metrics**. If there are multiple evaluation metrics, compute sample size for each of them\n",
    "    - step 5: collect total traffic, decide what proportion of total traffic should be exposed to the expriement and compute the duration of the exposure.\n",
    "        - compute the duration for the largest sample size\n",
    "        - if with maximal possible traffic, the duration to collect sufficient sample size is still too long, need to delete or modify this evaluation metric. \n",
    "    - step 6: decide when to conduct experiment and collect experiment results\n",
    "    - step 7: sanity check\n",
    "    - step 8: hypothesis tests (single and multiple; parametric and sign tests)\n",
    "    - step 9: slicing metrics and draw conclusions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Details on Metrics Design\n",
    "1. Basic Rules:\n",
    "    - Be practical. Make sure you can actually collect this metric\n",
    "    - Consider repeatance. For example, if use user_id for click through rate, one user_id can click multiple times. But if we use cookie, which is unique for each click/user_id, there won't be repeatance\n",
    "    - Avoid metrics that need a long time to collect, e.g. customer reviews after change of design of a shopping website\n",
    "2. Methods:\n",
    "    - step 1: Be clear with your objective, e.g. better user experience, more revenue.\n",
    "    - step 2: Understand your business model and construct a **customer funnel**. For example, for social media app, which aims to increase users' engagement in the community, the customer funnel is: visitor --> lurker --> voter --> content creator --> moderator --> group creator\n",
    "    - step 3: Use metrics to detail the customer funnel. For example, for the social media customer funnel metrics can be: # clicks on shared links/# downloads of the app --> # signed up users --> # users that at least liked/shared/saved a post --> # users at least created a comment/post --> # users joined at least one group --> # users created at least one group\n",
    "    - step 3.5: potential measurement units:\n",
    "        - pageviews\n",
    "        - clicks\n",
    "        - user_id/email/phone number\n",
    "        - cookie\n",
    "        - device\n",
    "        - event\n",
    "    - step 4: After creating measurable metrics according to the funnel, we can divide the metrics between two stages to create new metrics that measure the conversion rate\n",
    "        - click-through-rate: $\\frac{\\#~clicks}{\\#~pagevisits}$\n",
    "        - click through-probability: $\\frac{\\#~\\text{unique users who clicks}}{\\#~\\text{unique users who visited the page}}$\n",
    "    - step 5: Further detail above created metrics.\n",
    "        - Check whether the measurement is practical.\n",
    "        - Decide the time interval to collect the metrics, e.g. considering seasonality, collect user_time_spent weekly, forming weekly_user_total/avg_time_spent\n",
    "        - Slicing. e.g. collect weekday_user_avg_time_spent vs. weekend_user_avg_time_spent, US_user_avg_time_spent vs CN_user_avg_time_spent, student_avg_time_spent vs. employee_avg_time_spent\n",
    "    - step 6: Interpret the metrics and check for validity:\n",
    "        - Check whether the magnitude makes sense. For example, 10% click-though-rate is probably wrong\n",
    "        - Check the distribution of metrics by different slicing (time/region/group/platform/android vs. ios) and understand the difference. For example, difference in loading time between PC and mobile platforms is normal and due to the difference in platform infrustractures.\n",
    "3. Other techniques:\n",
    "    - use external data, e.g. open sources customer/market information\n",
    "    - use own data, e.g. retrospective/observational data, survey/user experience data, collect new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Details on experiment Design\n",
    "1. Unit of Diversion\n",
    "\n",
    "| unit of diversion | pros | cons |\n",
    "| ---- | ---- | ---- |\n",
    "| user_id | stable | multiple id for one person | \n",
    "| cookie | unique by brower; can avoid multiple id to some extend | changes when you change browser; users can clear cookies |\n",
    "| event | use only for non-user-visible changes | no consistent experience | \n",
    "| device_id | unchangeable by user | less common; only available for mobile tied to specific devices, e.g. iwatch |\n",
    "| IP address | stable and unique most of the time | less common; changes when location changes | \n",
    "\n",
    "2. Decide Target Population. For example:\n",
    "    - users on certain browser/platform/system(android vs. ios)\n",
    "    - users in certain geo-region\n",
    "    - users in certain language\n",
    "    - user in certain age bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Details on experiment Analysis\n",
    "1. Sanity check\n",
    "    - purpose: use invariant metrics to check whether the control group and experiment group are equally separated.\n",
    "    - hypothesis: $H_0: p_{select~in~control} = 0.5$\n",
    "    - Required information from expriement: total sample sizes $N$, control group sample size $n_c$\n",
    "    - How to test:\n",
    "        - Step 1: under null hypothesis, $SE = \\sqrt{\\frac{0.5*0.5}{N}}$\n",
    "        - Step 2: build 95% confidence interval for plausible probability of being selected to control group: $[0.5 - 1.96*SE, 0.5 + 1.96*SE]$\n",
    "        - Step 3: compute observed select-in-control-probability: $\\hat p_{select~in~control} = \\frac{n_c}{N}$. If $\\hat p_{select~in~control}\\in [0.5 - 1.96*SE, 0.5 + 1.96*SE]$, sanity check passed.\n",
    "        - Step 3.5: if there are multiple invariant metrics, conduct sanity check for each invariant metric.\n",
    "2. Parametric hypothesis test\n",
    "    - hypothesis: $H_0: p_c = p_e$ or $H_0: \\mu_c = \\mu_e$ where $p_c$ and $p_e$ are probability of binary evaluation metrics in control group and experiment group, respectively. $\\mu_c$ and $\\mu_e$ are mean of continuous evaluation metrics in control group and experiment group, respectively.\n",
    "    - standard error: \n",
    "        - For continuous evaluation metrics: for known within-group standard deviation: $$SE = \\sqrt{\\frac{\\sigma_c^2}{n_c} + \\frac{\\sigma_e^2}{n_e}}$$ when within-group standard deviations are unknown: $$SE = \\sigma_{pool}\\sqrt{\\frac{1}{n_c} + \\frac{1}{n_e}}$$, where $\\sigma^2_{pool} = \\frac{\\sum_{i=1}^{n_c}(v_{ci}-\\mu_c)^2 + \\sum_{i=1}^{n_e}(v_{ei} - \\mu_e)^2}{N-2}$ is the pooled standard error.\n",
    "        - For binary evaluation metrics: $$SE = \\sqrt{p_{pool}(1-p_{pool})\\left(\\frac{1}{n_c}+\\frac{1}{n_e}\\right)}$$ where $p_{pool} = \\frac{p_cn_c + p_en_e}{N}$ is the probability of event in the pooled sample\n",
    "    - construct 95% confidence intervals: $[\\mu_e-\\mu_c - 1.96 SE, \\mu_e-\\mu_c + 1.96 SE]$ or $[p_e-p_c - 1.96 SE, p_e-p_c + 1.96 SE]$ \n",
    "    - significance check:\n",
    "        - statistical significnace: compare the confidence interval with 0\n",
    "        - practical significance: compare the confidence interval with minimal accepted change in magenitude\n",
    "3. Sign test\n",
    "    - hypothesis: $H_0: p_+ = 0.5$ where $p_+ = \\frac{sum_{i=1}^{min(n_c, n_e)}I_i(v_{ei} > v_{ci})}{min(n_c, n_e)}$ is the proportion of paried control-experiment results where there is an improved experiment result.\n",
    "    - p-value: $$p-value = pbinom(sum_{i=1}^{min(n_c, n_e)}I_i(v_{ei} > v_{ci}), 0.5, min(n_c, n_e))$$\n",
    "    - significance check: compare p-value with 0.05 or $\\alpha_{pertest}$ if Bonferroni correction is used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
